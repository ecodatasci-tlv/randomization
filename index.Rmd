---
title: "Randomization and Null Models"
subtitle: "EcoDataSci-TLV"
date: "2019-03-13"
output:
  xaringan::moon_reader:
    lib_dir: libs
    css: ["default", "custom-fonts.css"]
    nature:
      highlightStyle: github
      highlightLines: true
      ratio: '16:9'
      countIncrementalSlides: false
---

```{r setup, include=FALSE}
library(knitr)
library(tidyverse)
library(ggpubr)
conflicted::conflict_prefer("filter", "dplyr")

options(htmltools.dir.version = FALSE,
        tibble.width = 70,
        ggplot2.continuous.colour = "viridis",
        ggplot2.continuous.fill = "viridis")
opts_chunk$set(
  fig.width = 6.5,
  fig.height = 4.5,
  fig.align = "center",
  cache = TRUE
)
theme_set(theme_minimal() +
            theme(text = element_text(size = 20))) 
```

# What is randomization

---

# Make your randomization reproducible

```{r}
set.seed(20180313)
```

--

```{r}
snacks <- c("pretzels", "cookies", "popcorn", "bamba", "carrots")
sample(snacks, 3)
```

_If you use the same seed, you should be getting the same results as us!_

---
# Sampling with replacement

```{r error = TRUE}
sample(snacks, 10)
```

--

```{r}
sample(snacks, 10, replace = TRUE)
```

---

# Sample probabilities

```{r}
sample(snacks, 10, replace = TRUE,
       prob = c(.8, .05, .05, .05, .05))
```

.center[.img-small[
![](img/pretzel.jpg)
]]

---

# Shortcut: use an integer instead of a vector

```{r}
sample(5)
sample(1:5)
```

---
class: center, inverse, middle

# `?Distributions`
---

# Types of distributions

- **d**xxx - density/mass function
- **p**xxx - cumulative distribution function
- **q**xxx - quantile function
- **r**xxx - random generation

---

# Uniform distribution 

```{r}
tibble(x = runif(1000)) %>% #<<
  ggplot(aes(x)) + geom_histogram(bins = 50)
```

---
# Normal distribution

```{r}
tibble(x = rnorm(1000)) %>% #<<
  ggplot(aes(x)) + geom_histogram(bins = 50)
```

---
# Binomial distribution

```{r}
tibble(x = rbinom(1000, 1, prob = .7)) %>% #<<
  ggplot(aes(x)) + geom_histogram(bins = 50)
```

---
# Bootstrapping

.center[
![](https://wilkelab.org/ungeviz/articles/sampling-bootstrapping_files/figure-html/bootstrap-demo-1.gif)
]

[ungeviz](https://wilkelab.org/ungeviz/index.html) by Claus Wilke
---
class: middle, center, inverse

# Null models
---

# What are null models
---
# How you construct them
---
# What to do/what to look out for
---
# Biases:

1. Narcissistic effect - incorporate the thing you are trying to test into your model
2. Jack Horner - you detect something that you should have taken into account but didn't

---
# Example:
Do grey Pokemon have higher defense values than other pokemon?
```{r message=FALSE}
pokemon <- read_csv("data/pokemon.csv")
```

We can run a simple t-test to find out:
```{r}
pokemon <- pokemon %>%
    mutate(color2 = fct_collapse(
        color,
        Grey = c("Grey"),
        Not_grey = c("Black", "Blue", "Brown", "Green", "Pink", "Purple", "Red", "White", "Yellow")
    ))
```
---
```{r}
t.test(defense ~ color2, pokemon)
```
The answer is: yes, they do.
---
# Now let's do the same thing but with a null model instead!
1) A numeric value: how many observations are in our group of interest?
```{r}
n <- length(which(pokemon$color2 == "Grey"))
```
2) A vector containing the pooled observations.
```{r}
x <- pokemon$defense
```
3) An empty vector in the length of however many iterations we want to run.
```{r}
t <- vector(length = 10000)
```
---
4. This is our null model - for each iteration, we randomly sample n values out of x, without replacements. This gives us a simulated "grey" group, where pokemon are assigned randomly. We then calculate a t-score for that group. After running this process for t iterations, we get a null distribution of t-scores.
```{r}
for (i in 1:length(t)){
    grey <- sample(x, n, replace = F)
    not_grey <- setdiff(x, grey)
    t[i] <- (mean(grey) - mean(not_grey)) / (sqrt(var(grey)/length(grey)))
}
```
---
Now that we have a null distribution, we can calculate the observed t-score:
```{r}
pokemon_sum <- pokemon %>%
    group_by(color2) %>%
    summarise(
        mean = mean(defense),
        se = sqrt(var(defense))/n()
    )
obs <- (pokemon_sum[2,2] - pokemon_sum[1,2])/pokemon_sum[2,3]
```
And of course, a p-value.
This is calculated as the precentage of t-scores in the null distribution as extreme, or more extreme, than the observed t-score.
```{r}
p.val<-(length(which(abs(t) >= abs(as.numeric(obs)))) / length(t))
p.val
```
---
Now let's visualize this. We'll plot the histogram of the null distribution, and mark our observed value with a red line: We can see here very clearly that our observed t-score is very, very extreme.
```{r}
data_frame(null = t, obs = obs$mean) %>%
    ggplot() +
    geom_density(aes(null)) + 
    geom_vline(xintercept = obs[[1]], color = "red")
```
---

Now we can start adding complications. For instance, you may recall that _grey_ Pokemon tend to be _steel_ type.

.center[
![](img/Steelix.png)
![](img/Skarmory.png)
]
So how do we incorporate this into our null model?
---
# Weighted sampling
One of the things we can do is to make sure, when we construct our model, to make the sampling from the pool not random, but weighted according to another variable.

In this case, we would give a higher probability to selecting a steel type Pokemon when generating our "grey" group.
How much higher?

In this case we would base it on the distribution of types of *all* grey Pokemon, to make sure our simulated "grey" group has a similar distribution of types as our observed group.
---
```{r}
chi_data <- table(pokemon$type_1, pokemon$color2)
grey_prop <- data_frame(
    type_1 = row.names(chi_data),
    prop = chi_data[,2]/sum(chi_data[,2])
)
pokemon <- merge(grey_prop, pokemon, by = "type_1")

n <- length(which(pokemon$color2 == "Grey"))
x <- pokemon$defense
t2 <- vector(length = 10000)
prop <- pokemon$prop

for (i in 1:length(t)){
    grey <- sample(x,
                   n,
                   replace = F,
                   prob = prop)
    not_grey <- setdiff(x, grey)
    t2[i] <- (mean(grey) - mean(not_grey)) / (sqrt(var(grey)/length(grey)))
}

```
---
```{r}
pokemon_sum <- pokemon %>%
    group_by(color2) %>%
    summarise(
        mean = mean(defense),
        se = sqrt(var(defense))/n()
    )
obs <- (pokemon_sum[2,2] - pokemon_sum[1,2])/pokemon_sum[2,3]

p.val<-(length(which(abs(t2) >= abs(as.numeric(obs)))) / length(t2))
p.val
```
---
```{r}
data_frame(null = t2, obs = obs$mean) %>%
    ggplot() +
    geom_density(aes(null)) + 
    geom_vline(xintercept = obs[[1]], color = "red")
```
In this case the results are basically the same, still a significant difference.
---
But if we look more closely, we can see that the null distributions actually differ slightly from one another:
```{r}
data_frame(random = t, weighted = t2) %>%
    gather("model", "scores") %>%
    ggdensity(x = "scores",
              fill = "model")
```
---
- pokemon dataset: t.test versus null model
- slight complication...pick and choose how you construct your model

Are blue pokemon more likely to have high attack ratings?
If 20% of pokemon are blue, you would randomly select 20% 1000x. Compare observed group to random groups

Are blue pokemon more likely to be water pokemon?
Weight sampling to make water more likely
---
# Take home

Null models are powerful but you have to be careful about how you construct them
You have to write them yourselves!

---

# Other stuff

- cross-validation & friends
