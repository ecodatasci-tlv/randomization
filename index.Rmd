---
title: "Randomization and Null Models"
subtitle: "EcoDataSci-TLV"
date: "2019-03-13"
output:
  xaringan::moon_reader:
    lib_dir: libs
    css: ["default", "custom-fonts.css"]
    nature:
      highlightStyle: github
      highlightLines: true
      ratio: '16:9'
      countIncrementalSlides: false
---

```{r setup, include=FALSE}
library(knitr)
library(tidyverse)
library(ggpubr)
conflicted::conflict_prefer("filter", "dplyr")

options(htmltools.dir.version = FALSE,
        tibble.width = 70,
        ggplot2.continuous.colour = "viridis",
        ggplot2.continuous.fill = "viridis")
opts_chunk$set(
  fig.width = 6.5,
  fig.height = 4.5,
  fig.align = "center",
  cache = TRUE
)
theme_set(theme_minimal() +
            theme(text = element_text(size = 20))) 
```

# What is randomization

---

# Make your randomization reproducible

```{r}
set.seed(20180313)
```

--

```{r}
snacks <- c("pretzels", "cookies", "popcorn", "bamba", "carrots")
sample(snacks, 3)
```

_If you use the same seed, you should be getting the same results as us!_

---
# Sampling with replacement

```{r error = TRUE}
sample(snacks, 10)
```

--

```{r}
sample(snacks, 10, replace = TRUE)
```

---

# Sample probabilities

```{r}
sample(snacks, 10, replace = TRUE,
       prob = c(.8, .05, .05, .05, .05))
```

.center[.img-small[
![](img/pretzel.jpg)
]]

---

# Shortcut: use an integer instead of a vector

```{r}
sample(5)
sample(1:5)
```

---
class: center, inverse, middle

# `?Distributions`
---

# Types of distributions

- **d**xxx - density/mass function
- **p**xxx - cumulative distribution function
- **q**xxx - quantile function
- **r**xxx - random generation

---

# Uniform distribution 

```{r}
tibble(x = runif(1000)) %>% #<<
  ggplot(aes(x)) + geom_histogram(bins = 50)
```

---
# Normal distribution

```{r}
tibble(x = rnorm(1000)) %>% #<<
  ggplot(aes(x)) + geom_histogram(bins = 50)
```

---
# Binomial distribution

```{r}
tibble(x = rbinom(1000, 1, prob = .7)) %>% #<<
  ggplot(aes(x)) + geom_histogram(bins = 50)
```

---
# Bootstrapping

.center[
![](https://wilkelab.org/ungeviz/articles/sampling-bootstrapping_files/figure-html/bootstrap-demo-1.gif)
]

[ungeviz](https://wilkelab.org/ungeviz/index.html) by Claus Wilke
---
class: middle, center, inverse

# Null models
---
# What are null models?

_A null model is a pattern generating model that is based on randomization ofâ€¦data or random sampling from a known or imagined distribution. The null model is designed with respect to some ecological or evolutionary process of interest._

.pull-left[
![](img/Null.png)
]

.pull-right[
Gotelli & Graves 1996

_Null models in ecology_
]
---
# Why use them?

1) Assumptions of classical statistical tests are often not met

2) Random processes may result in non-random patterns

.center[
![](img/Mudkip.png)
]

---
# How to prepare your own null model in 5 easy steps!

.pull-left-big[
1. Collect data

2. Break the structure in the data. Replace the process of interest with a random draw

3. Repeat many, many, many times

4. Collate all the repetitions to generate a null distribution

5. Compare observed pattern to random pattern expected by chance
]

.pull-right-small[
.center[
![](img/MarthaStewart.jpg)
]
]
---
# Example:

Do grey Pokemon have higher defense values than other pokemon?

```{r message=FALSE}
pokemon <- read_csv("data/pokemon.csv")
```

We can run a simple t-test to find out:

```{r}
pokemon <- pokemon %>%
  mutate(color2 = if_else(color == "Grey", "Grey", "Not_grey"))
```
---
```{r}
t.test(defense ~ color2, pokemon)
```

The answer is: yes, they do.
---
# Now let's do it with a null model instead!

1) A numeric value: how many observations are in our group of interest?
```{r}
n_grey <- sum(pokemon$color2 == "Grey")
```

2) A vector containing the pooled observations.
```{r}
pooled_obs <- pokemon$defense
```

3) An empty vector in the length of however many iterations we want to run.
```{r}
t <- vector(length = 10000)
```

---

4) This is our null model - for each iteration, we randomly sample our `pooled_obs` to create simulated `grey` and `not_grey` groups, then calculate a t-score. After running this process for `t` iterations, we get a null distribution of t-scores.

```{r}
for (i in 1:length(t)){
  # sample WITHOUT replacement
  grey_idx <- sample(length(pooled_obs), n_grey, 
                     replace = FALSE)
  
  # create simulated grey & not_grey groups
  grey <- pooled_obs[grey_idx] 
  not_grey <- pooled_obs[-grey_idx]
  
  # calculate t-score
  t[i] <- t.test(grey, not_grey)$statistic
}
```

---

Now that we have a null distribution, we can calculate the **observed t-score**:

```{r}
(poke_sum <- pokemon %>%
   group_by(color2) %>%
   summarise(
     mean = mean(defense),
     se = sd(defense) / sqrt(n())
   ))
(obs <- (poke_sum$mean[2] - poke_sum$mean[1])/poke_sum$se[2])
```

---

And of course, a **p-value**.
This is calculated as the precentage of t-scores in the null distribution as extreme, or more extreme, than the observed t-score.

```{r}
p.val <- sum(abs(t) >= abs(obs)) / length(t)
p.val
```
---

Now let's visualize this. We'll plot the histogram of the null distribution, and mark our observed value with a red line: We can see clearly that our observed t-score is very, very extreme.

```{r}
tibble(scores = t, obs = obs) %>%
  ggplot() +
  geom_density(aes(scores)) + 
  geom_vline(xintercept = obs, color = "red")
```

---

Now we can start adding complications. For instance, you may recall that _grey_ Pokemon tend to be _steel_ type.

.center[
![](img/Steelix.png)
![](img/Skarmory.png)
]

How do we incorporate this into our null model?
---
# Weighted sampling

One of the things we can do is to construct our model with sampling that is _weighted_ according to another variable, rather than randomly.  

--
In this case, we would give a higher probability to selecting a steel type Pokemon when generating our "grey" group.

_How much higher?_ For this example, we can base the probability on the distribution of types of *all* grey Pokemon, to make sure our simulated "grey" group has a similar distribution of types as our observed group.
---
```{r}
grey_prop <- pokemon %>% 
   filter(color2 == "Grey") %>%
   count(type_1) %>% 
   mutate(prop = n/sum(n))

grey_prop
```
---

Let's add the proportions back into our data frame.

```{r}
pokemon <- left_join(pokemon, grey_prop, by = "type_1") %>% 
  mutate(prop = replace_na(prop, 0))
```

---
Now we can get our null distribution again and store it in a new variable, `t2`. The main change here is using `prop` for the sampling probabilities.

```{r}
n_grey <- sum(pokemon$color2 == "Grey")
pooled_obs <- pokemon$defense
t2 <- vector(length = 10000) 
prop <- pokemon$prop #<<
```

```{r}
for (i in 1:length(t2)){
  grey_idx <- sample(length(pooled_obs), n_grey, 
                     replace = FALSE, 
                     prob = prop) #<<
  
  grey <- pooled_obs[grey_idx] 
  not_grey <- pooled_obs[-grey_idx]
  
  t2[i] <- t.test(grey, not_grey)$statistic
}
```

---
```{r}
tibble(scores = t2, obs = obs) %>%
  ggplot() +
  geom_density(aes(scores)) + 
  geom_vline(xintercept = obs, color = "red")
```

In this case the results are basically the same, still a significant difference.
---
But if we look more closely, we can see that the null distributions actually differ slightly from one another:

```{r}
tibble(random = t, weighted = t2) %>%
  gather("model", "scores") %>%
  ggpubr::ggdensity(x = "scores", fill = "model")
```
---
# Biases

There are two main issues that can occur when constructing null models, and we should always be aware of them:

.center[
![](img/frustration.jpg)
]
---

# 1) Narcissus effect
### (Colwell & Winkler 1984)

.pull-left[
The null model algorithm includes the effect it was trying to omit.

## INFLATED TYPE II ERROR RATE!

_'Narcissus could not see the bottom of the pool for his own image, and could not guess its depth.'_
]

.bottom-right[
![](img/narcissus.jpg)
]
---
#2) Jack Horner effect
### (Wilson 1995)

.pull-left[
The null model algorithm omits effects it shouldn't.

##INFLATED TYPE I ERROR RATE!

_'Jack Horner...thought himself a good boy for demonstrating that plum pies contain plums.'_

]

.bottom-right[
![](img/jackhorner.jpg)
]
---
# Take home

Null models are powerful!!!

1) They can be tailor-made to fit any type of data/question;

2) They require _zero_ assumptions about the underlying structure of the data;

But you have to use your brain, even more than usual!

.bottom-right[
![](img/Psyduck.png)
]

---

# Other stuff

- cross-validation & friends
