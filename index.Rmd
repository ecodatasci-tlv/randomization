---
title: "Randomization and Null Models"
subtitle: "EcoDataSci-TLV"
date: "2019-03-13"
output:
  xaringan::moon_reader:
    lib_dir: libs
    css: ["default", "custom-fonts.css"]
    nature:
      highlightStyle: github
      highlightLines: true
      ratio: '16:9'
      countIncrementalSlides: false
---

```{r setup, include=FALSE}
library(knitr)
library(tidyverse)
library(ggpubr)
conflicted::conflict_prefer("filter", "dplyr")

options(htmltools.dir.version = FALSE,
        tibble.width = 70,
        ggplot2.continuous.colour = "viridis",
        ggplot2.continuous.fill = "viridis")
opts_chunk$set(
  fig.width = 6.5,
  fig.height = 4.5,
  fig.align = "center",
  cache = TRUE
)
theme_set(theme_minimal() +
            theme(text = element_text(size = 20))) 
```

# What is randomization

---

# Make your randomization reproducible

```{r}
set.seed(20180313)
```

--

```{r}
snacks <- c("pretzels", "cookies", "popcorn", "bamba", "carrots")
sample(snacks, 3)
```

_If you use the same seed, you should be getting the same results as us!_

---
# Sampling with replacement

```{r error = TRUE}
sample(snacks, 10)
```

--

```{r}
sample(snacks, 10, replace = TRUE)
```

---

# Sample probabilities

```{r}
sample(snacks, 10, replace = TRUE,
       prob = c(.8, .05, .05, .05, .05))
```

.center[.img-small[
![](img/pretzel.jpg)
]]

---

# Shortcut: use an integer instead of a vector

```{r}
sample(5)
sample(1:5)
```

---
class: center, inverse, middle

# `?Distributions`
---

# Types of distributions

- **d**xxx - density/mass function
- **p**xxx - cumulative distribution function
- **q**xxx - quantile function
- **r**xxx - random generation

---

# Uniform distribution 

```{r}
tibble(x = runif(1000)) %>% #<<
  ggplot(aes(x)) + geom_histogram(bins = 50)
```

---
# Normal distribution

```{r}
tibble(x = rnorm(1000)) %>% #<<
  ggplot(aes(x)) + geom_histogram(bins = 50)
```

---
# Binomial distribution

```{r}
tibble(x = rbinom(1000, 1, prob = .7)) %>% #<<
  ggplot(aes(x)) + geom_histogram(bins = 50)
```

---
# Bootstrapping

.center[
![](https://wilkelab.org/ungeviz/articles/sampling-bootstrapping_files/figure-html/bootstrap-demo-1.gif)
]

[ungeviz](https://wilkelab.org/ungeviz/index.html) by Claus Wilke
---
class: middle, center, inverse

# Null models
---
# What are null models?

_A null model is a pattern generating model that is based on randomization of…data or random sampling from a known or imagined distribution. The null model is designed with respect to some ecological or evolutionary process of interest’._

.pull-left[
![](img/Null.png)
]

.pull-right[
Gotelli & Graves 1996

_Null models in ecology_
]
---
# Why use them?

1) Assumptions of classical statistical tests are often not met.

2) Random processes may result in non-random patterns.

.center[
![](img/Mudkip.png)
]

---
# How to prepare your own null model in 5 easy steps!

.pull-left[
1) Collect data

2) Break the structure in the data by replacing process of interest with random draw

3) Repeat many, many, many times

4) Collate all the repetitions to generate a null distribution

5) Compare obaserved pattern to random pattern expected by chance
]

.bottom[
![](img/MarthaStewart.tif)
]
---
# Example:
Do grey Pokemon have higher defense values than other pokemon?
```{r message=FALSE}
pokemon <- read_csv("data/pokemon.csv")
```

We can run a simple t-test to find out:
```{r}
pokemon <- pokemon %>%
    mutate(color2 = fct_collapse(
        color,
        Grey = c("Grey"),
        Not_grey = c("Black", "Blue", "Brown", "Green", "Pink", "Purple", "Red", "White", "Yellow")
    ))
```
---
```{r}
t.test(defense ~ color2, pokemon)
```
The answer is: yes, they do.
---
# Now let's do the same thing but with a null model instead!
1) A numeric value: how many observations are in our group of interest?
```{r}
n <- length(which(pokemon$color2 == "Grey"))
```
2) A vector containing the pooled observations.
```{r}
x <- pokemon$defense
```
3) An empty vector in the length of however many iterations we want to run.
```{r}
t <- vector(length = 10000)
```
---
4) This is our null model - for each iteration, we randomly sample n values out of x, without replacements. This gives us a simulated "grey" group, where pokemon are assigned randomly. We then calculate a t-score for that group. After running this process for t iterations, we get a null distribution of t-scores.
```{r}
for (i in 1:length(t)){
    grey <- sample(x, n, replace = F)
    not_grey <- setdiff(x, grey)
    t[i] <- (mean(grey) - mean(not_grey)) / (sqrt(var(grey)/length(grey)))
}
```
---
Now that we have a null distribution, we can calculate the observed t-score:
```{r}
pokemon_sum <- pokemon %>%
    group_by(color2) %>%
    summarise(
        mean = mean(defense),
        se = sqrt(var(defense))/n()
    )
obs <- (pokemon_sum[2,2] - pokemon_sum[1,2])/pokemon_sum[2,3]
```
And of course, a p-value.
This is calculated as the precentage of t-scores in the null distribution as extreme, or more extreme, than the observed t-score.
```{r}
p.val<-(length(which(abs(t) >= abs(as.numeric(obs)))) / length(t))
p.val
```
---
Now let's visualize this. We'll plot the histogram of the null distribution, and mark our observed value with a red line: We can see here very clearly that our observed t-score is very, very extreme.
```{r}
data_frame(scores = t, obs = obs$mean) %>%
    ggplot() +
    geom_density(aes(scores)) + 
    geom_vline(xintercept = obs[[1]], color = "red")
```
---

Now we can start adding complications. For instance, you may recall that _grey_ Pokemon tend to be _steel_ type.

.center[
![](img/Steelix.png)
![](img/Skarmory.png)
]
So how do we incorporate this into our null model?
---
# Weighted sampling
One of the things we can do is to make sure, when we construct our model, to make the sampling from the pool not random, but weighted according to another variable.

In this case, we would give a higher probability to selecting a steel type Pokemon when generating our "grey" group.
How much higher?

For this example we can base it on the distribution of types of *all* grey Pokemon, to make sure our simulated "grey" group has a similar distribution of types as our observed group.
---
```{r}
chi_data <- table(pokemon$type_1, pokemon$color2)
grey_prop <- tibble(
    type_1 = row.names(chi_data),
    prop = chi_data[,2]/sum(chi_data[,2])
)
grey_prop
```
---
```{r}
pokemon <- merge(grey_prop, pokemon, by = "type_1")

n <- length(which(pokemon$color2 == "Grey"))
x <- pokemon$defense
t2 <- vector(length = 10000)
prop <- pokemon$prop

for (i in 1:length(t)){
    grey <- sample(x,
                   n,
                   replace = F,
                   prob = prop)
    not_grey <- setdiff(x, grey)
    t2[i] <- (mean(grey) - mean(not_grey)) / (sqrt(var(grey)/length(grey)))
}

```
---
```{r}
pokemon_sum <- pokemon %>%
    group_by(color2) %>%
    summarise(
        mean = mean(defense),
        se = sqrt(var(defense))/n()
    )
obs <- (pokemon_sum[2,2] - pokemon_sum[1,2])/pokemon_sum[2,3]

p.val<-(length(which(abs(t2) >= abs(as.numeric(obs)))) / length(t2))
p.val
```
---
```{r}
tibble(scores = t2, obs = obs$mean) %>%
    ggplot() +
    geom_density(aes(scores)) + 
    geom_vline(xintercept = obs[[1]], color = "red")
```
In this case the results are basically the same, still a significant difference.
---
But if we look more closely, we can see that the null distributions actually differ slightly from one another:
```{r}
tibble(random = t, weighted = t2) %>%
    gather("model", "scores") %>%
    ggdensity(x = "scores",
              fill = "model")
```
---
# Biases:

There are two main issues that can occur when constructing null models, and we should always be aware of them:

.bottom[
![](img/frustration.jpg)
]
---

# 1) Narcissus effect
### (Colwell & Winkler 1984)

.pull-left[
The null model algorithm includes the effect it was trying to omit.

##INFLATED TYPE II ERROR RATE!

_'Narcissus could not see the bottom of the pool for his own image, and could not guess its depth.'_
]

.bottom-right[
![](img/narcissus.jpg)
]
---
#2) Jack Horner effect
### (Wilson 1995)

.pull-left[
The null model algorithm omits effects it shouldn't.

##INFLATED TYPE I ERROR RATE!

_'Jack Horner...thought himself a good boy for demonstrating that plum pies contain plums.'_

]

.bottom-right[
![](img/jackhorner.jpg)
]
---
# Take home

Null models are powerful!!!

1) They can be tailor-made to fit any type of data/question;

2) They require _zero_ assumptions about the underlying structure of the data;

But you have to use your brain, even more than usual!

.bottom-right[
![](img/Psyduck.png)
]

---

# Other stuff

- cross-validation & friends
